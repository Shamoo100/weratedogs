{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report\n",
    "\n",
    "To begin the data wrangling process, I had to gather three sources of required data in different ways. For the first and simplest method, I downloaded a CSV file from the Udacity website and read it into a dataframe called `archive`. The second data source was a TSV file located at a URL provided by Udacity, so I used the `requests` library to download the file programatically. Then I read this file into dataframe called `predictions`. For the third and most complex method, I used the Twitter API to read each tweet's JSON data in its own line in a TXT file. Once the file had been saved, I was able to read it line by line using the `json` library to eventually create dataframe `tweet_data`.\n",
    "\n",
    "In the assessment process, I performed two types of assessment -- visual and programmatic. In the `archive` dataframe, many of the issues were related to incorrect extraction of names, ratings, and dog stages from the `text` column. Additionally, the data was not tidy in the `archive` and `predictions` dataframes, as there were multiple columns each for several variables. In the `tweet_data` dataframe, there was some missing data due to some tweets being deleted. However, I noted that it would not be possible to retrieve this data elsewhere. Overall, I also needed to narrow down each dataframe to only original content with images, as well as tidy the data by having one dataframe for each observational unit -- tweet data, dog data, and image predictions.\n",
    "\n",
    "After creating copies of the dataframe, I began the cleaning process, which involved multiple iterations of defining the cleaning action, coding, and then testing the result. I made the decision to first narrow down the dataframes to original content with images, as that would possibly eliminate some of the other issues. Then I started tidying the data; `archive_clean` and `predictions_clean` were easier to make tidy because the data was generally valid and accurate. However, before making `dogs_clean` tidy by separating it from `archive_clean`, I had to re-extract the names, ratings, and dog stages. This was the most difficult part of the cleaning process, and I acknowledge that there may still be some issues with `dogs_clean`, simply because it was not feasible to go through each tweet individually to verify the accuracy. Finally, after making the three new dataframes tidy, I was able to easily resolve any remaining quality issues.\n",
    "\n",
    "To end the data wrangling process, I saved each dataframe to its own CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
