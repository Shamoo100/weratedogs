{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report\n",
    "\n",
    "To begin the data wrangling process, I  gathered three sources of required data in different ways. For the first and simplest method, I downloaded a CSV file from the Udacity website and read it into a dataframe called `archive`. The second data source was a TSV file located at a URL provided by Udacity, so I used the `requests` library to download the file programatically. Then I read this file into dataframe called `predictions`. For the third and most complex method, I used the Twitter API to read each tweet's JSON data in its own line in a TXT file. Once the file had been saved, I was able to read it line by line using the `json` library to eventually create dataframe `tweet_data`.\n",
    "\n",
    "In the assessment process, I performed two types of assessment -- visual and programmatic. In the `archive` dataframe, many of the issues were related to incorrect extraction of names, ratings, and dog stages from the `text` column. Additionally, the data was not tidy in the `archive` and `predictions` dataframes, as there were multiple columns each for several variables. In the `tweet_data` dataframe, there was some missing data due to some tweets being deleted. However, I noted that it would not be possible to retrieve this data elsewhere. Overall, I also needed to narrow down each dataframe to only original content with images, as well as tidy the data by having one dataframe for each observational unit -- tweet data, dog data, and image predictions.\n",
    "\n",
    "After creating copies of the dataframe, I began the cleaning process, which involved multiple iterations of defining the cleaning action, coding, and then testing the result. I made the decision to first narrow down the dataframes to original content with images, as that would possibly eliminate some of the other issues. Then I started tidying the data; `archive_clean` and `predictions_clean` were easier to make tidy because the data was generally valid and accurate. However, before making `dogs_clean` tidy by separating it from `archive_clean`, I had to re-extract the names, ratings, and dog stages. This was the most difficult part of the cleaning process, and I acknowledge that there may still be some issues with `dogs_clean`, simply because it was not feasible to go through each tweet individually to verify the accuracy. Finally, after making the three new dataframes tidy, I was able to easily resolve any remaining quality issues.\n",
    "\n",
    "To end the data wrangling process, I saved each dataframe to its own CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report\n",
    "\n",
    "To start the data wrangling process, I obtained three sources of necessary data through various methods. The first method was to download a CSV file from the Udacity website and convert it into a dataframe called `archive`. The second method was to use the `requests` library to programmatically download a TSV file from a URL provided by Udacity, which was then converted into a dataframe called `predictions`. The third and most complicated method was to use the Twitter API to save each tweet's JSON data in its own line in a TXT file. Once the file was saved, I used the `json` library to read it line by line and create a dataframe called `tweet_data`.\n",
    "\n",
    "During the assessment phase, I conducted both visual and programmatic evaluations. In the 'archive' dataframe, issues included incorrect extraction of names, ratings, and dog stages from the `text` column, as well as a lack of tidiness in the `archive` and `predictions` dataframes, with multiple columns for several variables. In the `tweet_data` dataframe, there was some missing data due to deleted tweets, which could not be retrieved elsewhere. In addition, I needed to narrow down each dataframe to only original content with images, and make the data tidy by creating one dataframe for each observational unit - tweet data, dog data, and image predictions.\n",
    "\n",
    "After creating copies of the dataframes, I began the cleaning process, which involved multiple steps of defining the cleaning action, coding, and testing the results. I initially focused on narrowing down the dataframes to original content with images, in hopes of resolving other issues. Then, I tidied the data - `archive_clean` and `predictions_clean` were easier to make tidy, as the data was generally valid and accurate. However, before making `dogs_clean` tidy by separating it from `archive_clean`, I had to re-extract the names, ratings, and dog stages. This was the most challenging part of the cleaning process, and there may still be some inaccuracies in `dogs_clean` as it was not feasible to individually verify each tweet. Finally, after making the three new dataframes tidy, any remaining quality issues were easily resolved.\n",
    "\n",
    "To conclude the data wrangling process, I saved each dataframe as a separate CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
